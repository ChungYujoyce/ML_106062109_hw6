{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "from random import randrange\n",
    "import numpy as np \n",
    "\n",
    "ETA = 0.2\n",
    "attribute_number=4\n",
    "instance_number=90\n",
    "class_label_number=2\n",
    "\n",
    "def loadDataset(dataset, data=[]):\n",
    "    \n",
    "    newdata = []\n",
    "    for x in range(len(dataset)):\n",
    "        for i in range(0,len(dataset[x]),4):\n",
    "            if dataset[x][i] == \"I\":\n",
    "                newdata.append(dataset[x][i:len(dataset[x])-1])\n",
    "                break\n",
    "            else:\n",
    "                attribute = float(dataset[x][i:i+3])\n",
    "                newdata.append(attribute)\n",
    "            \n",
    "        data.append(newdata)\n",
    "        newdata=[]         \n",
    "\n",
    "\n",
    "class perceptron_learning():\n",
    "    def predict(self, row, weights):\n",
    "        activation = weights[0]\n",
    "        for i in range(len(row)-1):\n",
    "            activation += weights[i + 1] * row[i]\n",
    "        return 1.0 if activation >= 0.0 else 0.0\n",
    " \n",
    "    # Estimate Perceptron weights using stochastic gradient descent\n",
    "    def train_weights(self, train, weights, l_rate, n_epoch):\n",
    "        #weights = [0.0 for i in range(len(train[0]))]\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0.0\n",
    "            for row in train:\n",
    "                prediction = self.predict(row, weights)\n",
    "                error = row[-1] - prediction\n",
    "                sum_error += error**2\n",
    "                weights[0] = weights[0] + l_rate * error\n",
    "                for i in range(len(row)-1):\n",
    "                    weights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "            #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "            \n",
    "            #print(\"wei\",weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_NN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_set = []\n",
    "        self.class_labels = []  \n",
    "    \n",
    "    def add_to_training_set(self, example, class_label):\n",
    "        found = False\n",
    "        for i in range(len(self.class_labels)):\n",
    "            if self.class_labels[i] == class_label:\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            self.class_labels.append(class_label)\n",
    "        self.training_set.append(example)\n",
    "            \n",
    "    def calculate_error(self, example1, example2):\n",
    "        summ = 0.0\n",
    "        for i in range(len(example1)-1):\n",
    "            summ += pow((example1[i] - example2[i]), 2)\n",
    "        summ = math.sqrt(summ)\n",
    "        \n",
    "        return summ\n",
    "    \n",
    "    def classifier(self, example):\n",
    "        index = 0\n",
    "        error = 1000000\n",
    "        \n",
    "        for s in self.training_set:\n",
    "            new_error = self.calculate_error(s, example)\n",
    "            if error >= new_error:\n",
    "                index = s[-1]\n",
    "                error = new_error\n",
    "        return index\n",
    " \n",
    "    def getNeighbors(self, trainingSet, testInstance):\n",
    "        distances = []\n",
    "        length = len(testInstance)-1\n",
    "        print(length)\n",
    "        for x in range(len(trainingSet)):\n",
    "            dist = self.euclideanDistance(testInstance, trainingSet[x], length)\n",
    "            distances.append((trainingSet[x], dist))\n",
    "        distances.sort(key=operator.itemgetter(1))\n",
    "        neighbors = []\n",
    "        for x in range(3): # 3NN\n",
    "            neighbors.append(distances[x][0])\n",
    "        return neighbors\n",
    " \n",
    "    def getResponse(self, neighbors):\n",
    "        classVotes = {}\n",
    "        for x in range(len(neighbors)):\n",
    "            response = neighbors[x][-1]\n",
    "            if response in classVotes:\n",
    "                classVotes[response] += 1\n",
    "            else:\n",
    "                classVotes[response] = 1 \n",
    "        sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        return sortedVotes[0][0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaboost():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_set = []\n",
    "        self.test = []\n",
    "        self.class_labels = []\n",
    "        self.px = []\n",
    "        self.classifier_textbook = [] \n",
    "        self.classifier_original = []\n",
    "        self.Induce_Classifier_Textbook = [K_NN() for i in range(9)]# training_subset_number\n",
    "        self.Induce_Classifier_Original = [K_NN() for i in range(9)]\n",
    "\n",
    "    def add_to_training_set(self, example, class_label):\n",
    "        found = False\n",
    "        for i in range(len(self.class_labels)):\n",
    "            if self.class_labels[i] == class_label:\n",
    "                found = True\n",
    "                example.append(i)\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            example.append(len(self.class_labels))\n",
    "            self.class_labels.append(class_label)\n",
    "        self.training_set.append(example)\n",
    "        \n",
    "    def add_to_testset(self, example, class_label):\n",
    "        found = False\n",
    "        for i in range(len(self.class_labels)):\n",
    "            if self.class_labels[i] == class_label:\n",
    "                found = True\n",
    "                example.append(i)\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            example.append(len(self.class_labels))\n",
    "            self.class_labels.append(class_label)\n",
    "        self.test.append(example)\n",
    "        \n",
    "    def get_example(self):\n",
    "        d = random.random()\n",
    "        for i in range(len(self.px)):\n",
    "            if d < self.px[i]:\n",
    "                return self.training_set[i]\n",
    "            d -= self.px[i]\n",
    "            \n",
    "        return self.training_set[-1]\n",
    "    \n",
    "    def create_subset(self):\n",
    "        subset = []\n",
    "        for i in range(10): # element number in subset = 10\n",
    "            ex = self.get_example()\n",
    "            subset.append(ex)\n",
    "        return subset\n",
    "    \n",
    "    def subset_eval_textbook(self, Round):\n",
    "        epsilon = 0\n",
    "        BETA = 0\n",
    "        check = []\n",
    "        \n",
    "        for i in range(len(self.training_set)):\n",
    "            ex = self.training_set[i]\n",
    "            label = self.Induce_Classifier_Textbook[Round].classifier(ex)\n",
    "            if label != ex[attribute_number]: # misclassify\n",
    "                epsilon += self.px[i]\n",
    "                #print(\"example:\",i,\"misclassify\")\n",
    "            \n",
    "            check.append(label == ex[-1])\n",
    "\n",
    "        \n",
    "        print(\"error\",epsilon)\n",
    "        BETA = float(float(epsilon) / float(1-epsilon))\n",
    "        \n",
    "        for i in range(len(self.training_set)):\n",
    "            if check[i] == True:\n",
    "                self.px[i] *= BETA\n",
    "\n",
    "        summ = sum(self.px) # sum up p*error\n",
    "        self.px = [i / summ for i in self.px] # normalized\n",
    "            \n",
    "    def subset_eval_original(self, Round):\n",
    "        epsilon = 0\n",
    "        BETA = 0\n",
    "        check = []\n",
    "        \n",
    "        for i in range(len(self.training_set)):\n",
    "            ex = self.training_set[i]\n",
    "            label = self.Induce_Classifier_Original[Round].classifier(ex)\n",
    "            if label != ex[attribute_number]:\n",
    "                epsilon += self.px[i]\n",
    "                #print(\"example:\",i,\"misclassify\")\n",
    "            \n",
    "            check.append(label == ex[-1])\n",
    "        \n",
    "        print(\"error\",epsilon)\n",
    "        # different from textbook version\n",
    "        BETA = pow(float(float(epsilon) / float(1-epsilon)), 0.5)\n",
    "        \n",
    "        for i in range(len(self.training_set)):\n",
    "            if check[i]:\n",
    "                self.px[i] *= BETA\n",
    "            else:\n",
    "                self.px[i] /= BETA\n",
    "\n",
    "        summ = sum(self.px)\n",
    "        self.px = [i / summ for i in self.px]\n",
    "        \n",
    "        return epsilon\n",
    "            \n",
    "    def subset_classification_textbook(self, Round):\n",
    "        #print(\"induce classifier:\", Round, \"subset classification(text)\")\n",
    "        subset = self.create_subset()\n",
    "        for s in subset:\n",
    "            self.Induce_Classifier_Textbook[Round].add_to_training_set(s, self.class_labels[s[-1]])\n",
    "            \n",
    "        self.subset_eval_textbook(Round)\n",
    "        \n",
    "    def subset_classification_original(self, Round):\n",
    "        #print(\"induce classifier:\", Round, \"subset classification(origin)\")\n",
    "        subset = self.create_subset()\n",
    "        for s in subset:\n",
    "            self.Induce_Classifier_Original[Round].add_to_training_set(s, self.class_labels[s[-1]])\n",
    "            \n",
    "        epsilon = self.subset_eval_original(Round)\n",
    "        return epsilon\n",
    "        \n",
    "    def textbook_version_classify(self):\n",
    "        self.px = [float(1/len(self.training_set)) for i in range(len(self.training_set))]\n",
    "        epsilon = []\n",
    "        print(\"==========================textbook version==============================\")\n",
    "        for i in range(9):\n",
    "            epsilon.append(self.subset_classification_textbook(i)) # updating p\n",
    "        \n",
    "        classifier_weights = self.master_classifier_textbook(epsilon)\n",
    "        predict = self.master_classifier_textbook_predict(classifier_weights)\n",
    "\n",
    "        print(\"predict accuracy:\",(predict/float(len(self.test))) * 100.0)          \n",
    "            \n",
    "    def original_version_classify(self):\n",
    "        epsilon = 0\n",
    "        self.px = [float(1/len(self.training_set)) for i in range(len(self.training_set))]\n",
    "        print(\"==========================original version============================\")\n",
    "        for i in range(9):\n",
    "            epsilon = self.subset_classification_original(i)            \n",
    "        \n",
    "        classifier_weights = self.master_classifier_original(epsilon)\n",
    "        predict = self.master_classifier_original_predict(classifier_weights)\n",
    "\n",
    "        print(\"predict accuracy:\",(predict/float(len(self.test))) * 100.0)       \n",
    "    \n",
    "    def master_classifier_textbook(self, epsilon): # use perceptron update weights\n",
    "        classifier_textbook_W = [0.2 for i in range(9)]\n",
    "        # update classifiers weights\n",
    "        subset = self.create_subset()\n",
    "        classifier_textbook_W = perceptron_learning().train_weights(self.training_set, classifier_textbook_W, 0.2, 5)\n",
    "        print(\"textbook classifier weights\", classifier_textbook_W)\n",
    "        correct = 0\n",
    "        for i in range(len(self.training_set)):\n",
    "            pos, neg = 0.0, 0.0\n",
    "            example = self.training_set[i]\n",
    "            for j in range(9):\n",
    "                subset_classifier_label = self.Induce_Classifier_Textbook[j].classifier(example)\n",
    "                if(subset_classifier_label):\n",
    "                    pos += classifier_textbook_W[j]\n",
    "                else:\n",
    "                    neg += classifier_textbook_W[j]\n",
    "            \n",
    "            master_label = 0\n",
    "            master_label = 1 if pos > neg else 0\n",
    "            \n",
    "            if(example[-1] == master_label):\n",
    "                correct +=1\n",
    "\n",
    "        print(\"textbook training accuracy:\", (correct/float(len(self.training_set))) * 100.0)\n",
    "        return classifier_textbook_W\n",
    "    \n",
    "    def master_classifier_textbook_predict(self, classifier_W):\n",
    "        correct = 0\n",
    "        for i in range(len(self.test)):\n",
    "            pos, neg = 0.0, 0.0\n",
    "            example = self.test[i]\n",
    "            for j in range(9):\n",
    "                subset_classifier_label = self.Induce_Classifier_Textbook[j].classifier(example)\n",
    "                if(subset_classifier_label):\n",
    "                    pos += classifier_W[j]\n",
    "                else:\n",
    "                    neg += classifier_W[j]\n",
    "            \n",
    "            master_label = 0\n",
    "            master_label = 1 if pos > neg else 0\n",
    "            \n",
    "            if(example[-1] == master_label):\n",
    "                correct += 1\n",
    "\n",
    "        return correct\n",
    "    \n",
    "    def master_classifier_original(self, epsilon):\n",
    "        # create wieghts without updating\n",
    "        classifier_original_W = [0.5*np.log((1 - epsilon) / epsilon) for i in range(9)]\n",
    "        print(\"original classifier weights\", classifier_original_W)\n",
    "        correct = 0\n",
    "        for i in range(len(self.training_set)):\n",
    "            pos, neg = 0.0, 0.0\n",
    "            example = self.training_set[i]\n",
    "            for j in range(9):\n",
    "                subset_classifier_label = self.Induce_Classifier_Original[j].classifier(example)\n",
    "                if(subset_classifier_label):\n",
    "                    pos += classifier_original_W[j]\n",
    "                else:\n",
    "                    neg += classifier_original_W[j]\n",
    "            \n",
    "            master_label = 0\n",
    "            master_label = 1 if pos > neg else 0 # voting\n",
    "            \n",
    "            if(example[-1] == master_label):\n",
    "                correct += 1          \n",
    "                \n",
    "        print(\"original training accuracy:\", (correct/float(len(self.training_set))) * 100.0)\n",
    "        return classifier_original_W\n",
    "                    \n",
    "    def master_classifier_original_predict(self, classifier_W):\n",
    "        correct = 0\n",
    "        for i in range(len(self.test)):\n",
    "            pos, neg = 0.0, 0.0\n",
    "            example = self.test[i]\n",
    "            for j in range(9):\n",
    "                subset_classifier_label = self.Induce_Classifier_Original[j].classifier(example)\n",
    "                if(subset_classifier_label):\n",
    "                    pos += classifier_W[j]\n",
    "                else:\n",
    "                    neg += classifier_W[j]\n",
    "            \n",
    "            master_label = 0\n",
    "            master_label = 1 if pos > neg else 0\n",
    "            \n",
    "            if(example[-1] == master_label):\n",
    "                correct += 1\n",
    "\n",
    "        return correct\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class origin_perceptron():\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.weights = [0.2 for i in range(len(train[0]))]\n",
    "        \n",
    "    def process_label(self, data):\n",
    "        for row in data:\n",
    "            if row[-1] == \"Iris-setosa\":\n",
    "                row.pop(-1)\n",
    "                row.append(0)\n",
    "            else:\n",
    "                row.pop(-1)\n",
    "                row.append(1)\n",
    "        #print(data)\n",
    "        return data\n",
    "\n",
    "    def predict(self, row, weights):\n",
    "        activation = weights[0]\n",
    "        for i in range(len(row)-1):\n",
    "            activation += weights[i + 1] * row[i]\n",
    "        return 1.0 if activation >= 0.0 else 0.0\n",
    "    \n",
    "    # Calculate accuracy percentage\n",
    "    def accuracy_metric(self, actual, predicted):\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual)) * 100.0\n",
    "\n",
    "    # Evaluate an algorithm using a cross validation split\n",
    "    def evaluate_algorithm(self, dataset, n_folds, *args):\n",
    "        self.train = self.process_label(self.train)\n",
    "        folds = self.cross_validation_split(self.train, n_folds)\n",
    "        scores = list()\n",
    "        for fold in folds:\n",
    "            train_set = list(folds)\n",
    "            train_set.remove(fold)\n",
    "            train_set = sum(train_set, [])\n",
    "            test_set = list()\n",
    "            for row in fold:\n",
    "                row_copy = list(row)\n",
    "                test_set.append(row_copy)\n",
    "                row_copy[-1] = None\n",
    "            predicted= self.perceptron(train_set, test_set, *args)\n",
    "            actual = [row[-1] for row in fold]\n",
    "            accuracy = self.accuracy_metric(actual, predicted)\n",
    "            scores.append(accuracy)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def cross_validation_split(self, dataset, n_folds):\n",
    "        dataset_split = list()\n",
    "        dataset_copy = list(dataset)\n",
    "        fold_size = int(len(dataset) / n_folds)\n",
    "        for i in range(n_folds):\n",
    "            fold = list()\n",
    "            while len(fold) < fold_size:\n",
    "                index = randrange(len(dataset_copy))\n",
    "                fold.append(dataset_copy.pop(index))\n",
    "            dataset_split.append(fold)\n",
    "        return dataset_split\n",
    "\n",
    "    def train_weights(self, train, learning_rate, n_epoch):\n",
    "        for epoch in range(n_epoch):\n",
    "            sum_error = 0.0\n",
    "            for row in train:\n",
    "                prediction = self.predict(row, self.weights)\n",
    "                #print(prediction)\n",
    "                error = row[-1] - prediction #list indices must be integers or slices, not list\n",
    "                sum_error += error**2\n",
    "                self.weights[0] = self.weights[0] + learning_rate * error\n",
    "                for i in range(len(row)-1):\n",
    "                    self.weights[i+1] = self.weights[i+1] + learning_rate * error * row[i]\n",
    "            #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, learning_rate, sum_error))\n",
    "            \n",
    "        print(self.weights)\n",
    "        return self.weights \n",
    "\n",
    "\n",
    "    def perceptron(self, train, test, l_rate, n_epoch):\n",
    "        predictions = list()\n",
    "        self.weights = self.train_weights(train, l_rate, n_epoch)\n",
    "        for row in test:\n",
    "            prediction = self.predict(row, self.weights)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def predict_test(self, test, l_rate, n_epoch):\n",
    "        test = self.process_label(test)\n",
    "        predictions = list()\n",
    "        scores = list()\n",
    "        print(\"w\",self.weights)\n",
    "        for row in test:\n",
    "            prediction = self.predict(row, self.weights)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        actual = [row[-1] for row in test]\n",
    "        \n",
    "        print(\"s\",predictions)\n",
    "        accuracy = self.accuracy_metric(actual, predictions)\n",
    "        scores.append(accuracy)\n",
    "        \n",
    "        print('perceptron predict accuracy:', (sum(scores)/float(len(scores))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    trainingSet=[]\n",
    "    testSet=[]\n",
    "    \n",
    "    f = open('training-data.txt', \"r\")\n",
    "    train = f.readlines()\n",
    "    train = list(train)\n",
    "    f = open('testing-data.txt', \"r\")\n",
    "    test = f.readlines()\n",
    "    test = list(test)\n",
    "\n",
    "    loadDataset(train, trainingSet)\n",
    "    loadDataset(test, testSet)\n",
    "    \n",
    "    classifier = adaboost()\n",
    "    Linear_classifier = origin_perceptron(trainingSet, testSet)\n",
    "    \n",
    "    for i in range(90):\n",
    "        classifier.add_to_training_set(trainingSet[i][:4], trainingSet[i][-1])\n",
    "        \n",
    "    for i in range(10):\n",
    "        classifier.add_to_testset(testSet[i][:4], testSet[i][-1])\n",
    "        \n",
    "    classifier.original_version_classify()\n",
    "    classifier.textbook_version_classify()\n",
    "    print(\"==========================perceptron version============================\")\n",
    "    scores = Linear_classifier.evaluate_algorithm(trainingSet, 9, 0.2, 50)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('perceptron training accuracy:', (sum(scores)/float(len(scores))))\n",
    "    Linear_classifier.predict_test(testSet, 0.2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================original version============================\n",
      "error 0.044444444444444446\n",
      "error 0.36627906976744234\n",
      "error 0.11482452308140353\n",
      "error 0.2423613791219912\n",
      "error 0.3506752475385558\n",
      "error 0.29272373755623776\n",
      "error 0.2601670812394492\n",
      "error 0.40749132470938815\n",
      "error 0.3499108547867282\n",
      "original classifier weights [0.309715539267759, 0.309715539267759, 0.309715539267759, 0.309715539267759, 0.309715539267759, 0.309715539267759, 0.309715539267759, 0.309715539267759, 0.309715539267759]\n",
      "original training accuracy: 96.66666666666667\n",
      "predict accuracy: 80.0\n",
      "==========================textbook version==============================\n",
      "error 0.044444444444444446\n",
      "error 0.13953488372093006\n",
      "error 0.35641891891891897\n",
      "error 0.28387298744055073\n",
      "error 0.2513416232795562\n",
      "error 0.6348992345522765\n",
      "error 0.21791181730966103\n",
      "error 0.25706388015046494\n",
      "error 0.22987028376509874\n",
      "textbook classifier weights [0.4, 1.82, 0.7599999999999993, 3.56, 1.26, 0.2, 0.2, 0.2, 0.2]\n",
      "textbook training accuracy: 84.44444444444444\n",
      "predict accuracy: 90.0\n",
      "==========================perceptron version============================\n",
      "[-2.4, 0.07999999999999052, -1.2399999999999949, 3.0400000000000027, 2.2199999999999904]\n",
      "[-2.6, -2.560000000000016, -0.7199999999999914, 5.860000000000024, 4.899999999999979]\n",
      "[-3.400000000000001, -2.1400000000000197, -0.059999999999986064, 3.18000000000002, 3.5599999999999623]\n",
      "[-3.8000000000000007, -1.9800000000000169, 1.1600000000000166, 5.2400000000000375, 1.8199999999999557]\n",
      "[-6.000000000000003, -1.2000000000000226, 0.2600000000000232, 5.000000000000044, 0.7199999999999578]\n",
      "[-7.400000000000004, -2.2800000000000247, 2.0000000000000298, 6.96000000000003, 0.4599999999999588]\n",
      "[-6.600000000000003, -2.2800000000000273, -0.2799999999999695, 7.940000000000052, 0.5399999999999587]\n",
      "[-7.200000000000004, -0.28000000000002645, -1.6999999999999673, 3.0200000000000653, 1.999999999999959]\n",
      "[-7.200000000000004, -0.8800000000000261, 0.7200000000000375, 5.340000000000076, -3.81084053202585e-14]\n",
      "Scores: [90.0, 90.0, 100.0, 100.0, 100.0, 90.0, 80.0, 90.0, 100.0]\n",
      "perceptron training accuracy: 93.33333333333333\n",
      "w [-7.200000000000004, -0.8800000000000261, 0.7200000000000375, 5.340000000000076, -3.81084053202585e-14]\n",
      "s [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "perceptron predict accuracy: 90.0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
